{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d63437",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "BIO_LABELS = [\"PAD\", \"O\", \"B-GENOTYPE\", \"I-GENOTYPE\"]\n",
    "\n",
    "BASE_PATH = \"/home/msc23dhruv/msc23dhruvvol1claim/models/\"\n",
    "\n",
    "# config = {\"lr\": tune.loguniform(5e-5, 1e-4), \"batch_size\": tune.choice([2, 4, 8])}\n",
    "config = {\"lr\": [3e-5], \"batch_size\": [8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e17bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train_df = pd.read_pickle(\"../data/bert/normal/train.df\")\n",
    "normal_test_df = pd.read_pickle(\"../data/bert/normal/test.df\")\n",
    "normal_val_df = pd.read_pickle(\"../data/bert/normal/val.df\")\n",
    "\n",
    "normal_train_df_tokens = normal_train_df[\"tokens\"].tolist()\n",
    "normal_train_df_labels = normal_train_df[\"labels\"].tolist()\n",
    "\n",
    "normal_val_df_tokens = normal_val_df[\"tokens\"].tolist()\n",
    "normal_val_df_labels = normal_val_df[\"labels\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2318ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open(\"../data/biored_gene_ner.train.json\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse each line as a separate JSON object\n",
    "bioRed_data = []\n",
    "for line in lines:\n",
    "    json_obj = json.loads(line)\n",
    "    bioRed_data.append(json_obj)\n",
    "\n",
    "# Extract the features into separate arrays\n",
    "bioRed_input_ids = [obj[\"input_ids\"] for obj in bioRed_data]\n",
    "bioRed_token_type_ids = [obj[\"token_type_ids\"] for obj in bioRed_data]\n",
    "bioRed_attention_mask = [obj[\"attention_mask\"] for obj in bioRed_data]\n",
    "bioRed_labels = [obj[\"labels\"] for obj in bioRed_data]\n",
    "\n",
    "\n",
    "(\n",
    "    bioRed_input_ids_train_df,\n",
    "    bioRed_input_ids_test_df,\n",
    "    bioRed_token_type_ids_train_df,\n",
    "    bioRed_token_type_ids_test_df,\n",
    "    bioRed_attention_mask_train_df,\n",
    "    bioRed_attention_mask_test_df,\n",
    "    bioRed_labels_train_df,\n",
    "    bioRed_labels_test_df\n",
    ") = train_test_split(\n",
    "    bioRed_input_ids, bioRed_token_type_ids, bioRed_attention_mask, bioRed_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_span_f1(true_labels, predicted_labels):\n",
    "    true_spans = get_spans(true_labels)\n",
    "    predicted_spans = get_spans(predicted_labels)\n",
    "    true_entities = set(true_spans)\n",
    "    predicted_entities = set(predicted_spans)\n",
    "\n",
    "    true_positive = len(true_entities.intersection(predicted_entities))\n",
    "    false_positive = len(predicted_entities - true_entities)\n",
    "    false_negative = len(true_entities - predicted_entities)\n",
    "\n",
    "    precision, recall, f1_score = calculate_precision_recall_f1(\n",
    "        true_positive, false_positive, false_negative\n",
    "    )\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def get_spans(labels):\n",
    "    spans = []\n",
    "    start = None\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 2:\n",
    "            if start is not None:\n",
    "                spans.append((start, i))\n",
    "            start = i\n",
    "        elif label == 3:\n",
    "            if start is None:\n",
    "                start = i\n",
    "        else:\n",
    "            if start is not None:\n",
    "                spans.append((start, i))\n",
    "                start = None\n",
    "    if start is not None:\n",
    "        spans.append((start, len(labels)))\n",
    "    return spans\n",
    "\n",
    "\n",
    "def calculate_precision_recall_f1(true_positives, false_positives, false_negatives):\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    if true_positives + false_negatives == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    if precision == 0 or recall == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "\n",
    "# Convert the tokens and labels into the format required by the BERT model\n",
    "def tokens_to_ids(tokens, labels):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    label_ids = []\n",
    "    for token_list, label_list in zip(tokens, labels):\n",
    "        input_ids.append(tokenizer.convert_tokens_to_ids(token_list))\n",
    "        attention_mask = [1] * len(token_list)\n",
    "        attention_masks.append(attention_mask)\n",
    "        label_id = [BIO_LABELS.index(label) for label in label_list]\n",
    "        label_ids.append(label_id)\n",
    "\n",
    "    input_ids = pad_sequence(\n",
    "        [torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=0.0\n",
    "    ).tolist()\n",
    "    attention_masks = pad_sequence(\n",
    "        [torch.tensor(seq) for seq in attention_masks],\n",
    "        batch_first=True,\n",
    "        padding_value=0.0,\n",
    "    ).tolist()\n",
    "    label_ids = pad_sequence(\n",
    "        [torch.tensor(seq) for seq in label_ids], batch_first=True, padding_value=0.0\n",
    "    ).tolist()\n",
    "\n",
    "    # Convert the data into PyTorch tensors\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "    label_ids = torch.tensor(label_ids, dtype=torch.long)\n",
    "    return input_ids, attention_masks, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec31346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    val_labels,\n",
    "    model,\n",
    "    optimizer,\n",
    "    epochs\n",
    "):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            batch_input_ids = batch[0].to(device)\n",
    "            batch_attention_masks = batch[1].to(device)\n",
    "            batch_labels = batch[2].to(device)\n",
    "            batch_labels = batch_labels.view(\n",
    "                -1, batch_labels.size(1)\n",
    "            )  # Reshape labels tensor\n",
    "            optimizer.zero_grad()\n",
    "            model.to(device)\n",
    "            outputs = model(\n",
    "                batch_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=batch_attention_masks,\n",
    "                labels=batch_labels,\n",
    "            )\n",
    "            total_train_loss += outputs[0].item()\n",
    "            outputs[0].backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_loss.append(avg_train_loss)\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_preds = []\n",
    "        for batch in val_dataloader:\n",
    "            batch_input_ids = batch[0].to(device)\n",
    "            batch_attention_masks = batch[1].to(device)\n",
    "            batch_labels = batch[2].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    batch_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=batch_attention_masks,\n",
    "                    labels=batch_labels,\n",
    "                )\n",
    "            total_val_loss += outputs[0].item()\n",
    "            logits = outputs[1].detach().cpu().numpy()\n",
    "            val_preds.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_loss.append(avg_val_loss)\n",
    "        val_preds = [p for pred in val_preds for p in pred]\n",
    "        val_labels_1 = [l for label in val_labels for l in label]\n",
    "        macro_f1 = f1_score(val_labels_1, val_preds, average=\"macro\")\n",
    "        weight_f1 = f1_score(val_labels_1, val_preds, average=\"weighted\")\n",
    "        print(val_labels_1, val_preds)\n",
    "        span_f1 = calculate_span_f1(val_labels_1, val_preds)\n",
    "        print(\n",
    "            \"Epoch:\",\n",
    "            epoch + 1,\n",
    "            \"Train Loss:\",\n",
    "            avg_train_loss,\n",
    "            \"Val Loss:\",\n",
    "            avg_val_loss,\n",
    "            \"F1 Score (Macro):\",\n",
    "            macro_f1,\n",
    "            \"F1 Score (Weighted):\",\n",
    "            weight_f1,\n",
    "            \"F1 Score (Span):\",\n",
    "            span_f1,\n",
    "        )\n",
    "    return span_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    normal_train_df_input_ids,\n",
    "    normal_train_df_attention_masks,\n",
    "    normal_train_df_label_ids,\n",
    ") = tokens_to_ids(normal_train_df_tokens, normal_train_df_labels)\n",
    "(\n",
    "    normal_val_df_input_ids,\n",
    "    normal_val_df_attention_masks,\n",
    "    normal_val_df_label_ids,\n",
    ") = tokens_to_ids(normal_val_df_tokens, normal_val_df_labels)\n",
    "\n",
    "normal_train_data = TensorDataset(\n",
    "    bioRed_input_ids_train_df,\n",
    "    normal_train_df_attention_masks,\n",
    "    normal_train_df_label_ids,\n",
    ")\n",
    "normal_train_sampler = RandomSampler(normal_train_data)\n",
    "\n",
    "\n",
    "normal_val_data = TensorDataset(\n",
    "    normal_val_df_input_ids, normal_val_df_attention_masks, normal_val_df_label_ids\n",
    ")\n",
    "normal_val_sampler = SequentialSampler(normal_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af163051",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioRed_train_data = TensorDataset(\n",
    "    bioRed_input_ids_train_df,\n",
    "    bioRed_attention_mask_train_df,\n",
    "    bioRed_labels_train_df,\n",
    ")\n",
    "bioRed_train_sampler = RandomSampler(bioRed_train_data)\n",
    "\n",
    "\n",
    "bioRed_val_data = TensorDataset(\n",
    "    bioRed_input_ids_test_df, bioRed_attention_mask_test_df, bioRed_labels_test_df\n",
    ")\n",
    "bioRed_val_sampler = SequentialSampler(bioRed_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findHyperParameters(config, normal_train_data, normal_train_sampler, normal_val_data, normal_val_sampler, model=None):\n",
    "    train_dataloader = DataLoader(\n",
    "        normal_train_data, sampler=normal_train_sampler, batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        normal_val_data, sampler=normal_val_sampler, batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "    if model:\n",
    "        model_normal = torch.load(\"/home/msc23dhruv/msc23dhruvvol1claim/model.pth\")\n",
    "    else:\n",
    "        model_normal = BertForTokenClassification.from_pretrained(\n",
    "            \"bert-base-uncased\", num_labels=len(BIO_LABELS)\n",
    "        )\n",
    "    optimizer_normal = torch.optim.Adam(model_normal.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    f1 = train_model(\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        normal_val_df_label_ids,\n",
    "        model_normal,\n",
    "        optimizer_normal,\n",
    "        epochs=5\n",
    "    )\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7561438",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, best_f1 = None, 0\n",
    "for params in ParameterGrid(config, bioRed_train_data, bioRed_train_sampler, bioRed_val_data, bioRed_val_sampler):\n",
    "    f1 = findHyperParameters(params)\n",
    "    if f1 > best_f1:\n",
    "        best_params = params\n",
    "        best_f1 = f1\n",
    "print(f\"{best_params=}\")\n",
    "print(f\"{best_f1=:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trained_model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=len(BIO_LABELS)\n",
    ")\n",
    "best_trained_model.to(device)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    normal_train_data,\n",
    "    sampler=normal_train_sampler,\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    normal_val_data,\n",
    "    sampler=normal_val_sampler,\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    best_trained_model.parameters(), lr=best_params[\"lr\"]\n",
    ")\n",
    "print(\"*************** NORMAL ********************************\")\n",
    "train_model(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    normal_val_df_label_ids,\n",
    "    best_trained_model,\n",
    "    optimizer,\n",
    "    epochs=5,\n",
    ")\n",
    "torch.save(best_trained_model, \"/home/msc23dhruv/msc23dhruvvol1claim/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefac21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, best_f1 = None, 0\n",
    "for params in ParameterGrid(config, normal_train_data, normal_train_sampler, normal_val_data, normal_val_sampler, best_trained_model):\n",
    "    f1 = findHyperParameters(params)\n",
    "    if f1 > best_f1:\n",
    "        best_params = params\n",
    "        best_f1 = f1\n",
    "print(f\"{best_params=}\")\n",
    "print(f\"{best_f1=:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trained_model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=len(BIO_LABELS)\n",
    ")\n",
    "best_trained_model.to(device)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    normal_train_data,\n",
    "    sampler=normal_train_sampler,\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    normal_val_data,\n",
    "    sampler=normal_val_sampler,\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    best_trained_model.parameters(), lr=best_params[\"lr\"]\n",
    ")\n",
    "print(\"*************** NORMAL ********************************\")\n",
    "train_model(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    normal_val_df_label_ids,\n",
    "    best_trained_model,\n",
    "    optimizer,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189317a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39cb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26627d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
